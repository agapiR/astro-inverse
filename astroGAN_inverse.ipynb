{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Used: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f97ccc9de50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import *\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device Used: \" + str(device))\n",
    "\n",
    "# set random seed\n",
    "manual_seed = 17 #42\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Signal  9000\n",
      "#Measurements  223\n",
      "(9000, 223)\n",
      "float32\n",
      "#Signal  9000\n",
      "#Measurements  205\n",
      "(9000, 205)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "## Data and Experiment Config\n",
    "\n",
    "# Set Result Directory\n",
    "# this folder's name should describe the configuration\n",
    "result_folder = './WGAN'\n",
    "if not os.path.exists(result_folder):\n",
    "    os.mkdir(result_folder)\n",
    "\n",
    "# Load Data\n",
    "data_folder = \"./Data/Dataset_5_35\"\n",
    "training_data_complete = np.load(osp.join(data_folder, \"spectra_complete_training.npy\")).astype(np.float32) #\"log_training.npy\"\n",
    "NLA_max = 205 \n",
    "training_data = training_data_complete[:,:NLA_max] #manually delete very high wavelengths\n",
    "TRAINING_DATA = training_data.shape[0]\n",
    "FEATURE_SIZE = training_data.shape[1]\n",
    "print(\"#Signal \", TRAINING_DATA)\n",
    "print(\"#Measurements \", FEATURE_SIZE)\n",
    "\n",
    "\n",
    "# Hyperparameters (could receive as arguments along with data/res directory)\n",
    "LATENT_SIZE = 50\n",
    "\n",
    "BATCH_SIZE = 100 #32,64,128\n",
    "NUM_EPOCHS = 5000\n",
    "D_ITERS = 1\n",
    "\n",
    "LOG_EVERY = 10   #error logging\n",
    "SAVE_EVERY = 100  #checkpoints and model saving\n",
    "\n",
    "LR_D = 0.01\n",
    "LR_G = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Model size (total): 0.251M\n",
      "Label convention:\n",
      "real: 1  fake: 0\n",
      "Architecture:\n",
      "Generator(\n",
      "  (layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=50, out_features=101, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): BatchNorm1d(101, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=101, out_features=153, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): BatchNorm1d(153, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=153, out_features=204, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): BatchNorm1d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=204, out_features=256, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=307, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): BatchNorm1d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Linear(in_features=307, out_features=205, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Model Definition\n",
    "\n",
    "# ~~toy code without dataloaders\n",
    "\n",
    "# Initialize model\n",
    "d = 6 #num layers\n",
    "netG = Generator(nz=LATENT_SIZE, nf=FEATURE_SIZE, num_hidden_layers=d).to(device)\n",
    "netD = Discriminator(nz=LATENT_SIZE, nf=FEATURE_SIZE, num_hidden_layers=d).to(device)\n",
    "model_size = sum(p.numel() for p in netG.parameters())\n",
    "print(\"Generative Model size (total): {:.3f}M\".format(model_size/1e6))\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "#netG.apply(weights_init)\n",
    "#netD.apply(weights_init)\n",
    "\n",
    "\n",
    "## Training Config\n",
    "\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "D_label_smoothing = 0.005\n",
    "print(\"Label convention:\")\n",
    "print(\"real: \"+ str(real_label) + \"  fake: \" + str(fake_label))\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=LR_D, weight_decay=0.001) #SGD(netD.parameters(), lr=LR_D)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=LR_G, weight_decay=0.001) #, betas=(beta1, 0.999))\n",
    "clip_value = 100\n",
    "\n",
    "# Learning rate decay\n",
    "decayRate = 0.5\n",
    "step = 1000 #decay every 1000 epochs\n",
    "lr_scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer=optimizerD, step_size=step, gamma=decayRate)\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer=optimizerG, step_size=step, gamma=decayRate)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='mean') #nn.SoftMarginLoss(reduction='mean'),nn.BCELoss(reduction='mean')\n",
    "sigmoid_response = nn.Sigmoid()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(30, LATENT_SIZE, device=device)\n",
    "\n",
    "print(\"Architecture:\")\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Epoch[1/2000]\tLoss_D: 1.0556\tLoss_G: 6.9914\tD(x): 0.6232\tD(G(z)): 0.4122 / 0.0961\n",
      "Epoch[10/2000]\tLoss_D: 20.2988\tLoss_G: 1.5156\tD(x): 0.6564\tD(G(z)): 0.9282 / 0.5360\n",
      "Epoch[20/2000]\tLoss_D: 1.1398\tLoss_G: 0.8238\tD(x): 0.6375\tD(G(z)): 0.4612 / 0.4429\n",
      "Epoch[30/2000]\tLoss_D: 1.0949\tLoss_G: 0.9283\tD(x): 0.6629\tD(G(z)): 0.4408 / 0.4152\n",
      "Epoch[40/2000]\tLoss_D: 2.6874\tLoss_G: 530.1486\tD(x): 0.7255\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[50/2000]\tLoss_D: 0.9274\tLoss_G: 18.5595\tD(x): 0.8147\tD(G(z)): 0.5013 / 0.0000\n",
      "Epoch[60/2000]\tLoss_D: 0.0576\tLoss_G: 4.7849\tD(x): 0.9597\tD(G(z)): 0.0129 / 0.0155\n",
      "Epoch[70/2000]\tLoss_D: 0.0443\tLoss_G: 4.8281\tD(x): 0.9796\tD(G(z)): 0.0135 / 0.0135\n",
      "Epoch[80/2000]\tLoss_D: 0.0050\tLoss_G: 5.8531\tD(x): 0.9996\tD(G(z)): 0.0045 / 0.0075\n",
      "Epoch[90/2000]\tLoss_D: 0.0271\tLoss_G: 6.8314\tD(x): 0.9787\tD(G(z)): 0.0051 / 0.0030\n",
      "Epoch[100/2000]\tLoss_D: 0.0296\tLoss_G: 6.4845\tD(x): 0.9847\tD(G(z)): 0.0125 / 0.0023\n",
      "Epoch[110/2000]\tLoss_D: 0.0054\tLoss_G: 7.2110\tD(x): 0.9959\tD(G(z)): 0.0012 / 0.0017\n",
      "Epoch[120/2000]\tLoss_D: 0.0002\tLoss_G: 27.3716\tD(x): 0.9998\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[130/2000]\tLoss_D: 0.0002\tLoss_G: 12.2981\tD(x): 0.9998\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[140/2000]\tLoss_D: 0.0009\tLoss_G: 17.4905\tD(x): 0.9991\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[150/2000]\tLoss_D: 0.0635\tLoss_G: 4.6079\tD(x): 0.9782\tD(G(z)): 0.0357 / 0.0172\n",
      "Epoch[160/2000]\tLoss_D: 0.0820\tLoss_G: 4.9189\tD(x): 0.9542\tD(G(z)): 0.0130 / 0.0133\n",
      "Epoch[170/2000]\tLoss_D: 7.6048\tLoss_G: 3.0317\tD(x): 0.2790\tD(G(z)): 0.0008 / 0.1120\n",
      "Epoch[180/2000]\tLoss_D: 0.0696\tLoss_G: 6.7950\tD(x): 0.9456\tD(G(z)): 0.0127 / 0.0041\n",
      "Epoch[190/2000]\tLoss_D: 0.5638\tLoss_G: 5.5805\tD(x): 0.7643\tD(G(z)): 0.0619 / 0.0257\n",
      "Epoch[200/2000]\tLoss_D: 0.0004\tLoss_G: 10.1094\tD(x): 0.9998\tD(G(z)): 0.0002 / 0.0001\n",
      "Epoch[210/2000]\tLoss_D: 0.0110\tLoss_G: 9.0031\tD(x): 0.9895\tD(G(z)): 0.0004 / 0.0004\n",
      "Epoch[220/2000]\tLoss_D: 0.0425\tLoss_G: 8.7680\tD(x): 0.9653\tD(G(z)): 0.0063 / 0.0063\n",
      "Epoch[230/2000]\tLoss_D: 0.0457\tLoss_G: 12.3924\tD(x): 0.9641\tD(G(z)): 0.0073 / 0.0040\n",
      "Epoch[240/2000]\tLoss_D: 0.2423\tLoss_G: 4.5412\tD(x): 0.9763\tD(G(z)): 0.1229 / 0.0329\n",
      "Epoch[250/2000]\tLoss_D: 1.3700\tLoss_G: 0.7086\tD(x): 0.5150\tD(G(z)): 0.4959 / 0.4946\n",
      "Epoch[260/2000]\tLoss_D: 1.3635\tLoss_G: 0.7073\tD(x): 0.5107\tD(G(z)): 0.4943 / 0.4937\n",
      "Epoch[270/2000]\tLoss_D: 1.3677\tLoss_G: 0.8589\tD(x): 0.4523\tD(G(z)): 0.4345 / 0.4237\n",
      "Epoch[280/2000]\tLoss_D: 0.1391\tLoss_G: 5.1955\tD(x): 0.8829\tD(G(z)): 0.0115 / 0.0101\n",
      "Epoch[290/2000]\tLoss_D: 0.0292\tLoss_G: 7.1358\tD(x): 0.9828\tD(G(z)): 0.0095 / 0.0062\n",
      "Epoch[300/2000]\tLoss_D: 0.0267\tLoss_G: 6.7540\tD(x): 0.9817\tD(G(z)): 0.0064 / 0.0066\n",
      "Epoch[310/2000]\tLoss_D: 0.0207\tLoss_G: 7.4645\tD(x): 0.9839\tD(G(z)): 0.0015 / 0.0014\n",
      "Epoch[320/2000]\tLoss_D: 0.1521\tLoss_G: 7.2791\tD(x): 0.9427\tD(G(z)): 0.0358 / 0.0040\n",
      "Epoch[330/2000]\tLoss_D: 0.0145\tLoss_G: 7.0580\tD(x): 0.9870\tD(G(z)): 0.0014 / 0.0012\n",
      "Epoch[340/2000]\tLoss_D: 0.0042\tLoss_G: 16.8359\tD(x): 0.9958\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[350/2000]\tLoss_D: 0.0106\tLoss_G: 8.5657\tD(x): 0.9898\tD(G(z)): 0.0003 / 0.0003\n",
      "Epoch[360/2000]\tLoss_D: 0.0093\tLoss_G: 8.9476\tD(x): 0.9918\tD(G(z)): 0.0010 / 0.0005\n",
      "Epoch[370/2000]\tLoss_D: 0.0070\tLoss_G: 9.6251\tD(x): 0.9934\tD(G(z)): 0.0003 / 0.0002\n",
      "Epoch[380/2000]\tLoss_D: 0.0048\tLoss_G: 9.4048\tD(x): 0.9954\tD(G(z)): 0.0001 / 0.0001\n",
      "Epoch[390/2000]\tLoss_D: 0.0101\tLoss_G: 8.1604\tD(x): 0.9904\tD(G(z)): 0.0005 / 0.0004\n",
      "Epoch[400/2000]\tLoss_D: 0.0009\tLoss_G: 8.6495\tD(x): 0.9994\tD(G(z)): 0.0002 / 0.0002\n",
      "Epoch[410/2000]\tLoss_D: 0.0126\tLoss_G: 9.4346\tD(x): 0.9876\tD(G(z)): 0.0001 / 0.0001\n",
      "Epoch[420/2000]\tLoss_D: 0.0100\tLoss_G: 9.4882\tD(x): 0.9902\tD(G(z)): 0.0002 / 0.0002\n",
      "Epoch[430/2000]\tLoss_D: 0.0098\tLoss_G: 8.7360\tD(x): 0.9905\tD(G(z)): 0.0003 / 0.0002\n",
      "Epoch[440/2000]\tLoss_D: 0.0131\tLoss_G: 8.7514\tD(x): 0.9873\tD(G(z)): 0.0003 / 0.0003\n",
      "Epoch[450/2000]\tLoss_D: 0.0132\tLoss_G: 7.4468\tD(x): 0.9885\tD(G(z)): 0.0016 / 0.0018\n",
      "Epoch[460/2000]\tLoss_D: 0.0081\tLoss_G: 10.2531\tD(x): 0.9923\tD(G(z)): 0.0003 / 0.0009\n",
      "Epoch[470/2000]\tLoss_D: 0.0095\tLoss_G: 9.7321\tD(x): 0.9910\tD(G(z)): 0.0004 / 0.0002\n",
      "Epoch[480/2000]\tLoss_D: 0.0003\tLoss_G: 10.6773\tD(x): 0.9998\tD(G(z)): 0.0001 / 0.0001\n",
      "Epoch[490/2000]\tLoss_D: 0.0008\tLoss_G: 8.8321\tD(x): 0.9995\tD(G(z)): 0.0002 / 0.0004\n",
      "Epoch[500/2000]\tLoss_D: 0.0067\tLoss_G: 9.3139\tD(x): 0.9943\tD(G(z)): 0.0009 / 0.0006\n",
      "Epoch[510/2000]\tLoss_D: 0.0332\tLoss_G: 6.0480\tD(x): 0.9747\tD(G(z)): 0.0060 / 0.0056\n",
      "Epoch[520/2000]\tLoss_D: 0.1921\tLoss_G: 8.6188\tD(x): 0.9145\tD(G(z)): 0.0371 / 0.0319\n",
      "Epoch[530/2000]\tLoss_D: 0.0265\tLoss_G: 8.5432\tD(x): 0.9765\tD(G(z)): 0.0023 / 0.0041\n",
      "Epoch[540/2000]\tLoss_D: 0.0675\tLoss_G: 6.3344\tD(x): 0.9673\tD(G(z)): 0.0157 / 0.0103\n",
      "Epoch[550/2000]\tLoss_D: 4.0074\tLoss_G: 39.6150\tD(x): 0.4004\tD(G(z)): 0.0397 / 0.0000\n",
      "Epoch[560/2000]\tLoss_D: 0.0148\tLoss_G: 6.9264\tD(x): 0.9873\tD(G(z)): 0.0020 / 0.0017\n",
      "Epoch[570/2000]\tLoss_D: 0.0267\tLoss_G: 6.7906\tD(x): 0.9757\tD(G(z)): 0.0022 / 0.0016\n",
      "Epoch[580/2000]\tLoss_D: 0.0117\tLoss_G: 11.8677\tD(x): 0.9894\tD(G(z)): 0.0008 / 0.0004\n",
      "Epoch[590/2000]\tLoss_D: 0.0127\tLoss_G: 11.4739\tD(x): 0.9887\tD(G(z)): 0.0012 / 0.0012\n",
      "Epoch[600/2000]\tLoss_D: 0.0247\tLoss_G: 6.0390\tD(x): 0.9827\tD(G(z)): 0.0064 / 0.0103\n",
      "Epoch[610/2000]\tLoss_D: 2.4082\tLoss_G: 3.7379\tD(x): 0.4701\tD(G(z)): 0.0049 / 0.1639\n",
      "Epoch[620/2000]\tLoss_D: 1.5103\tLoss_G: 8.8768\tD(x): 0.9307\tD(G(z)): 0.5530 / 0.1750\n",
      "Epoch[630/2000]\tLoss_D: 1.3602\tLoss_G: 0.7025\tD(x): 0.5249\tD(G(z)): 0.4997 / 0.4978\n",
      "Epoch[640/2000]\tLoss_D: 1.2856\tLoss_G: 0.7431\tD(x): 0.5493\tD(G(z)): 0.4802 / 0.4775\n",
      "Epoch[650/2000]\tLoss_D: 0.0201\tLoss_G: 17.6940\tD(x): 1.0000\tD(G(z)): 0.0193 / 0.0003\n",
      "Epoch[660/2000]\tLoss_D: 0.1485\tLoss_G: 9.3466\tD(x): 0.8760\tD(G(z)): 0.0065 / 0.0052\n",
      "Epoch[670/2000]\tLoss_D: 0.0309\tLoss_G: 14.3830\tD(x): 0.9850\tD(G(z)): 0.0143 / 0.0074\n",
      "Epoch[680/2000]\tLoss_D: 0.0953\tLoss_G: 16.8259\tD(x): 0.9745\tD(G(z)): 0.0436 / 0.0126\n",
      "Epoch[690/2000]\tLoss_D: 0.0006\tLoss_G: 18.1180\tD(x): 0.9996\tD(G(z)): 0.0002 / 0.0006\n",
      "Epoch[700/2000]\tLoss_D: 0.5080\tLoss_G: 5.7312\tD(x): 0.9654\tD(G(z)): 0.2164 / 0.0604\n",
      "Epoch[710/2000]\tLoss_D: 0.0029\tLoss_G: 7.7891\tD(x): 1.0000\tD(G(z)): 0.0029 / 0.0010\n",
      "Epoch[720/2000]\tLoss_D: 0.5153\tLoss_G: 249.7819\tD(x): 0.8316\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[730/2000]\tLoss_D: 0.0008\tLoss_G: 9.0754\tD(x): 0.9997\tD(G(z)): 0.0005 / 0.0004\n",
      "Epoch[740/2000]\tLoss_D: 0.0191\tLoss_G: 8.5201\tD(x): 0.9835\tD(G(z)): 0.0024 / 0.0014\n",
      "Epoch[750/2000]\tLoss_D: 0.0413\tLoss_G: 10.9414\tD(x): 0.9680\tD(G(z)): 0.0068 / 0.0062\n",
      "Epoch[760/2000]\tLoss_D: 1.6043\tLoss_G: 9.0350\tD(x): 0.6950\tD(G(z)): 0.3325 / 0.0743\n",
      "Epoch[770/2000]\tLoss_D: 1.2481\tLoss_G: 1.1237\tD(x): 0.6556\tD(G(z)): 0.5296 / 0.5048\n",
      "Epoch[780/2000]\tLoss_D: 1.3855\tLoss_G: 0.6999\tD(x): 0.4987\tD(G(z)): 0.4982 / 0.4967\n",
      "Epoch[790/2000]\tLoss_D: 1.3622\tLoss_G: 0.7178\tD(x): 0.5069\tD(G(z)): 0.4910 / 0.4936\n",
      "Epoch[800/2000]\tLoss_D: 0.3988\tLoss_G: 5.9296\tD(x): 0.7078\tD(G(z)): 0.0356 / 0.0360\n",
      "Epoch[810/2000]\tLoss_D: 0.0021\tLoss_G: 11.7785\tD(x): 0.9980\tD(G(z)): 0.0001 / 0.0001\n",
      "Epoch[820/2000]\tLoss_D: 0.3602\tLoss_G: 12.6706\tD(x): 0.9331\tD(G(z)): 0.1791 / 0.0459\n",
      "Epoch[830/2000]\tLoss_D: 0.0443\tLoss_G: 10.5451\tD(x): 0.9609\tD(G(z)): 0.0030 / 0.0001\n",
      "Epoch[840/2000]\tLoss_D: 1.6081\tLoss_G: 0.2907\tD(x): 0.8041\tD(G(z)): 0.7494 / 0.7479\n",
      "Epoch[850/2000]\tLoss_D: 1.5808\tLoss_G: 1.5286\tD(x): 0.5167\tD(G(z)): 0.2758 / 0.2590\n",
      "Epoch[860/2000]\tLoss_D: 1.5639\tLoss_G: 0.9267\tD(x): 0.6299\tD(G(z)): 0.5981 / 0.3974\n",
      "Epoch[870/2000]\tLoss_D: 1.3637\tLoss_G: 0.7441\tD(x): 0.5025\tD(G(z)): 0.4779 / 0.4753\n",
      "Epoch[880/2000]\tLoss_D: 0.6881\tLoss_G: 4.0358\tD(x): 0.7865\tD(G(z)): 0.2318 / 0.1356\n",
      "Epoch[890/2000]\tLoss_D: 1.2246\tLoss_G: 0.6397\tD(x): 0.7339\tD(G(z)): 0.5314 / 0.5289\n",
      "Epoch[900/2000]\tLoss_D: 1.3749\tLoss_G: 0.7037\tD(x): 0.5126\tD(G(z)): 0.5037 / 0.4951\n",
      "Epoch[910/2000]\tLoss_D: 0.2230\tLoss_G: 6.6986\tD(x): 0.8028\tD(G(z)): 0.0031 / 0.0020\n",
      "Epoch[920/2000]\tLoss_D: 0.0265\tLoss_G: 11.2719\tD(x): 0.9741\tD(G(z)): 0.0001 / 0.0000\n",
      "Epoch[930/2000]\tLoss_D: 0.0092\tLoss_G: 9.1529\tD(x): 0.9912\tD(G(z)): 0.0003 / 0.0002\n",
      "Epoch[940/2000]\tLoss_D: 0.0020\tLoss_G: 9.0737\tD(x): 0.9983\tD(G(z)): 0.0002 / 0.0002\n",
      "Epoch[950/2000]\tLoss_D: 0.0120\tLoss_G: 9.0321\tD(x): 0.9890\tD(G(z)): 0.0009 / 0.0009\n",
      "Epoch[960/2000]\tLoss_D: 0.0007\tLoss_G: 13.0706\tD(x): 0.9994\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[970/2000]\tLoss_D: 0.0012\tLoss_G: 10.2791\tD(x): 0.9989\tD(G(z)): 0.0002 / 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[980/2000]\tLoss_D: 0.0616\tLoss_G: 8.5835\tD(x): 0.9784\tD(G(z)): 0.0111 / 0.0016\n",
      "Epoch[990/2000]\tLoss_D: 0.2908\tLoss_G: 16.1616\tD(x): 0.8696\tD(G(z)): 0.0106 / 0.0359\n",
      "Epoch[1000/2000]\tLoss_D: 0.0303\tLoss_G: 10.8323\tD(x): 0.9705\tD(G(z)): 0.0003 / 0.0002\n",
      "Epoch[1010/2000]\tLoss_D: 0.0570\tLoss_G: 8.1054\tD(x): 0.9560\tD(G(z)): 0.0067 / 0.0087\n",
      "Epoch[1020/2000]\tLoss_D: 0.3145\tLoss_G: 3.5435\tD(x): 0.9012\tD(G(z)): 0.1519 / 0.0550\n",
      "Epoch[1030/2000]\tLoss_D: 1.4408\tLoss_G: 1.2038\tD(x): 0.6992\tD(G(z)): 0.5746 / 0.4262\n",
      "Epoch[1040/2000]\tLoss_D: 1.5651\tLoss_G: 0.4856\tD(x): 0.5784\tD(G(z)): 0.6250 / 0.6221\n",
      "Epoch[1050/2000]\tLoss_D: 0.8365\tLoss_G: 4.2335\tD(x): 0.5069\tD(G(z)): 0.0937 / 0.0802\n",
      "Epoch[1060/2000]\tLoss_D: 0.6741\tLoss_G: 5.0413\tD(x): 0.6955\tD(G(z)): 0.0867 / 0.0943\n",
      "Epoch[1070/2000]\tLoss_D: 0.0753\tLoss_G: 9.4763\tD(x): 0.9524\tD(G(z)): 0.0232 / 0.0035\n",
      "Epoch[1080/2000]\tLoss_D: 1.8939\tLoss_G: 1.0097\tD(x): 0.6129\tD(G(z)): 0.4450 / 0.4775\n",
      "Epoch[1090/2000]\tLoss_D: 0.6721\tLoss_G: 1.1538\tD(x): 0.5625\tD(G(z)): 0.0413 / 0.3284\n",
      "Epoch[1100/2000]\tLoss_D: 1.5730\tLoss_G: 9.5918\tD(x): 0.6720\tD(G(z)): 0.0482 / 0.0133\n",
      "Epoch[1110/2000]\tLoss_D: 0.3711\tLoss_G: 11.7328\tD(x): 0.8265\tD(G(z)): 0.0005 / 0.0004\n",
      "Epoch[1120/2000]\tLoss_D: 0.0870\tLoss_G: 7.1512\tD(x): 0.9474\tD(G(z)): 0.0283 / 0.0232\n",
      "Epoch[1130/2000]\tLoss_D: 0.0612\tLoss_G: 4.9037\tD(x): 0.9756\tD(G(z)): 0.0353 / 0.0084\n",
      "Epoch[1140/2000]\tLoss_D: 0.0319\tLoss_G: 8.3991\tD(x): 0.9767\tD(G(z)): 0.0080 / 0.0032\n",
      "Epoch[1150/2000]\tLoss_D: 1.0437\tLoss_G: 6.2265\tD(x): 0.9756\tD(G(z)): 0.6063 / 0.0260\n",
      "Epoch[1160/2000]\tLoss_D: 1.6100\tLoss_G: 1.0267\tD(x): 0.6514\tD(G(z)): 0.4232 / 0.3924\n",
      "Epoch[1170/2000]\tLoss_D: 1.4268\tLoss_G: 0.6368\tD(x): 0.5427\tD(G(z)): 0.5464 / 0.5441\n",
      "Epoch[1180/2000]\tLoss_D: 1.3748\tLoss_G: 0.7791\tD(x): 0.4939\tD(G(z)): 0.4625 / 0.4597\n",
      "Epoch[1190/2000]\tLoss_D: 0.7173\tLoss_G: 6.6357\tD(x): 0.6167\tD(G(z)): 0.0011 / 0.0019\n",
      "Epoch[1200/2000]\tLoss_D: 0.1044\tLoss_G: 6.3143\tD(x): 0.9161\tD(G(z)): 0.0154 / 0.0143\n",
      "Epoch[1210/2000]\tLoss_D: 0.0312\tLoss_G: 8.0774\tD(x): 0.9720\tD(G(z)): 0.0026 / 0.0070\n",
      "Epoch[1220/2000]\tLoss_D: 0.2166\tLoss_G: 11.6628\tD(x): 0.9549\tD(G(z)): 0.1438 / 0.0021\n",
      "Epoch[1230/2000]\tLoss_D: 0.0521\tLoss_G: 5.7788\tD(x): 0.9719\tD(G(z)): 0.0211 / 0.0088\n",
      "Epoch[1240/2000]\tLoss_D: 1.6366\tLoss_G: 0.3845\tD(x): 0.7065\tD(G(z)): 0.7004 / 0.6860\n",
      "Epoch[1250/2000]\tLoss_D: 1.1685\tLoss_G: 1.3114\tD(x): 0.5719\tD(G(z)): 0.2461 / 0.3634\n",
      "Epoch[1260/2000]\tLoss_D: 0.0000\tLoss_G: 12.0516\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[1270/2000]\tLoss_D: 0.0008\tLoss_G: 7.7333\tD(x): 0.9998\tD(G(z)): 0.0005 / 0.0005\n",
      "Epoch[1280/2000]\tLoss_D: 0.0014\tLoss_G: 8.8309\tD(x): 0.9988\tD(G(z)): 0.0002 / 0.0002\n",
      "Epoch[1290/2000]\tLoss_D: 0.0007\tLoss_G: 8.3518\tD(x): 0.9998\tD(G(z)): 0.0004 / 0.0003\n",
      "Epoch[1300/2000]\tLoss_D: 0.0017\tLoss_G: 70.5210\tD(x): 0.9983\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[1310/2000]\tLoss_D: 0.0033\tLoss_G: 11.6881\tD(x): 0.9971\tD(G(z)): 0.0004 / 0.0005\n",
      "Epoch[1320/2000]\tLoss_D: 0.0112\tLoss_G: 8.1921\tD(x): 0.9895\tD(G(z)): 0.0006 / 0.0011\n",
      "Epoch[1330/2000]\tLoss_D: 0.0132\tLoss_G: 7.3451\tD(x): 0.9878\tD(G(z)): 0.0007 / 0.0007\n",
      "Epoch[1340/2000]\tLoss_D: 0.0047\tLoss_G: 9.1757\tD(x): 0.9956\tD(G(z)): 0.0003 / 0.0006\n",
      "Epoch[1350/2000]\tLoss_D: 0.0032\tLoss_G: 10.9527\tD(x): 0.9972\tD(G(z)): 0.0003 / 0.0001\n",
      "Epoch[1360/2000]\tLoss_D: 0.0104\tLoss_G: 7.4322\tD(x): 0.9949\tD(G(z)): 0.0050 / 0.0052\n",
      "Epoch[1370/2000]\tLoss_D: 0.2197\tLoss_G: 7.0977\tD(x): 0.9781\tD(G(z)): 0.1192 / 0.0054\n",
      "Epoch[1380/2000]\tLoss_D: 0.2945\tLoss_G: 3.7677\tD(x): 0.8662\tD(G(z)): 0.0887 / 0.0819\n",
      "Epoch[1390/2000]\tLoss_D: 0.0484\tLoss_G: 7.6510\tD(x): 0.9607\tD(G(z)): 0.0080 / 0.0036\n",
      "Epoch[1400/2000]\tLoss_D: 1.8037\tLoss_G: 2.4252\tD(x): 0.5317\tD(G(z)): 0.2145 / 0.2079\n",
      "Epoch[1410/2000]\tLoss_D: 0.7420\tLoss_G: 40.4965\tD(x): 0.8593\tD(G(z)): 0.3466 / 0.0035\n",
      "Epoch[1420/2000]\tLoss_D: 2.7402\tLoss_G: 0.2877\tD(x): 0.9690\tD(G(z)): 0.9041 / 0.8108\n",
      "Epoch[1430/2000]\tLoss_D: 0.7288\tLoss_G: 1.6184\tD(x): 0.7169\tD(G(z)): 0.2689 / 0.2500\n",
      "Epoch[1440/2000]\tLoss_D: 1.3487\tLoss_G: 0.8283\tD(x): 0.5221\tD(G(z)): 0.4603 / 0.4434\n",
      "Epoch[1450/2000]\tLoss_D: 1.0286\tLoss_G: 1.0450\tD(x): 0.6158\tD(G(z)): 0.3587 / 0.3562\n",
      "Epoch[1460/2000]\tLoss_D: 1.3040\tLoss_G: 1.0265\tD(x): 0.4716\tD(G(z)): 0.3742 / 0.3647\n",
      "Epoch[1470/2000]\tLoss_D: 1.0653\tLoss_G: 1.1900\tD(x): 0.5591\tD(G(z)): 0.3223 / 0.3093\n",
      "Epoch[1480/2000]\tLoss_D: 2.4710\tLoss_G: 14.4845\tD(x): 0.3640\tD(G(z)): 0.1021 / 0.0000\n",
      "Epoch[1490/2000]\tLoss_D: 0.9815\tLoss_G: 2.2150\tD(x): 0.7037\tD(G(z)): 0.2300 / 0.1786\n",
      "Epoch[1500/2000]\tLoss_D: 0.0299\tLoss_G: 8.6790\tD(x): 0.9719\tD(G(z)): 0.0014 / 0.0009\n",
      "Epoch[1510/2000]\tLoss_D: 0.2561\tLoss_G: 12.5198\tD(x): 0.8637\tD(G(z)): 0.0189 / 0.0141\n",
      "Epoch[1520/2000]\tLoss_D: 0.0462\tLoss_G: 9.9935\tD(x): 0.9633\tD(G(z)): 0.0082 / 0.0018\n",
      "Epoch[1530/2000]\tLoss_D: 0.6306\tLoss_G: 6.5533\tD(x): 0.6627\tD(G(z)): 0.1575 / 0.1104\n",
      "Epoch[1540/2000]\tLoss_D: 1.3860\tLoss_G: 0.6974\tD(x): 0.5010\tD(G(z)): 0.4987 / 0.4979\n",
      "Epoch[1550/2000]\tLoss_D: 1.3894\tLoss_G: 0.7257\tD(x): 0.4883\tD(G(z)): 0.4855 / 0.4842\n",
      "Epoch[1560/2000]\tLoss_D: 0.2677\tLoss_G: 8.9114\tD(x): 0.8019\tD(G(z)): 0.0293 / 0.0442\n",
      "Epoch[1570/2000]\tLoss_D: 0.0547\tLoss_G: 7.1953\tD(x): 0.9514\tD(G(z)): 0.0046 / 0.0017\n",
      "Epoch[1580/2000]\tLoss_D: 0.0639\tLoss_G: 4.9396\tD(x): 0.9540\tD(G(z)): 0.0152 / 0.0117\n",
      "Epoch[1590/2000]\tLoss_D: 0.1713\tLoss_G: 6.0343\tD(x): 0.8780\tD(G(z)): 0.0274 / 0.0187\n",
      "Epoch[1600/2000]\tLoss_D: 0.3366\tLoss_G: 16.3685\tD(x): 0.7823\tD(G(z)): 0.0047 / 0.0059\n",
      "Epoch[1610/2000]\tLoss_D: 0.0747\tLoss_G: 6.2912\tD(x): 0.9575\tD(G(z)): 0.0193 / 0.0041\n",
      "Epoch[1620/2000]\tLoss_D: 1.2918\tLoss_G: 0.7394\tD(x): 0.5553\tD(G(z)): 0.4819 / 0.4781\n",
      "Epoch[1630/2000]\tLoss_D: 1.3180\tLoss_G: 0.7434\tD(x): 0.5373\tD(G(z)): 0.4800 / 0.4759\n",
      "Epoch[1640/2000]\tLoss_D: 4.5639\tLoss_G: 171.5505\tD(x): 0.2814\tD(G(z)): 0.9347 / 0.0000\n",
      "Epoch[1650/2000]\tLoss_D: 0.4834\tLoss_G: 3.0474\tD(x): 0.8101\tD(G(z)): 0.1620 / 0.1329\n",
      "Epoch[1660/2000]\tLoss_D: 43.3931\tLoss_G: 0.0301\tD(x): 0.9296\tD(G(z)): 1.0000 / 0.9703\n",
      "Epoch[1670/2000]\tLoss_D: 0.3666\tLoss_G: 2.5181\tD(x): 0.8424\tD(G(z)): 0.1581 / 0.1301\n",
      "Epoch[1680/2000]\tLoss_D: 0.0611\tLoss_G: 7.2597\tD(x): 0.9422\tD(G(z)): 0.0015 / 0.0013\n",
      "Epoch[1690/2000]\tLoss_D: 0.0181\tLoss_G: 7.3881\tD(x): 0.9861\tD(G(z)): 0.0026 / 0.0020\n",
      "Epoch[1700/2000]\tLoss_D: 0.0012\tLoss_G: 13.3993\tD(x): 0.9991\tD(G(z)): 0.0003 / 0.0002\n",
      "Epoch[1710/2000]\tLoss_D: 0.0028\tLoss_G: 10.3524\tD(x): 0.9976\tD(G(z)): 0.0004 / 0.0004\n",
      "Epoch[1720/2000]\tLoss_D: 0.0371\tLoss_G: 6.9157\tD(x): 0.9737\tD(G(z)): 0.0099 / 0.0042\n",
      "Epoch[1730/2000]\tLoss_D: 0.0183\tLoss_G: 7.9514\tD(x): 0.9825\tD(G(z)): 0.0007 / 0.0007\n",
      "Epoch[1740/2000]\tLoss_D: 0.0104\tLoss_G: 9.1881\tD(x): 0.9899\tD(G(z)): 0.0002 / 0.0002\n",
      "Epoch[1750/2000]\tLoss_D: 0.0009\tLoss_G: 9.6734\tD(x): 0.9992\tD(G(z)): 0.0001 / 0.0001\n",
      "Epoch[1760/2000]\tLoss_D: 0.2691\tLoss_G: 86.8174\tD(x): 0.8956\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[1770/2000]\tLoss_D: 0.0160\tLoss_G: 7.2743\tD(x): 0.9867\tD(G(z)): 0.0024 / 0.0024\n",
      "Epoch[1780/2000]\tLoss_D: 0.0492\tLoss_G: 6.4384\tD(x): 0.9644\tD(G(z)): 0.0052 / 0.0050\n",
      "Epoch[1790/2000]\tLoss_D: 0.0807\tLoss_G: 9.7008\tD(x): 0.9439\tD(G(z)): 0.0095 / 0.0032\n",
      "Epoch[1800/2000]\tLoss_D: 0.0973\tLoss_G: 9.7218\tD(x): 0.9513\tD(G(z)): 0.0271 / 0.0359\n",
      "Epoch[1810/2000]\tLoss_D: 0.8452\tLoss_G: 2.0372\tD(x): 0.7473\tD(G(z)): 0.2627 / 0.2221\n",
      "Epoch[1820/2000]\tLoss_D: 0.0714\tLoss_G: 6.4073\tD(x): 0.9510\tD(G(z)): 0.0182 / 0.0035\n",
      "Epoch[1830/2000]\tLoss_D: 0.5870\tLoss_G: 2.5297\tD(x): 0.7306\tD(G(z)): 0.1501 / 0.1264\n",
      "Epoch[1840/2000]\tLoss_D: 0.0878\tLoss_G: 6.2579\tD(x): 0.9515\tD(G(z)): 0.0204 / 0.0236\n",
      "Epoch[1850/2000]\tLoss_D: 1.3151\tLoss_G: 0.7299\tD(x): 0.5359\tD(G(z)): 0.4837 / 0.4824\n",
      "Epoch[1860/2000]\tLoss_D: 1.3906\tLoss_G: 0.7123\tD(x): 0.4950\tD(G(z)): 0.4943 / 0.4908\n",
      "Epoch[1870/2000]\tLoss_D: 0.2235\tLoss_G: 22.5148\tD(x): 0.8551\tD(G(z)): 0.0171 / 0.0100\n",
      "Epoch[1880/2000]\tLoss_D: 0.0015\tLoss_G: 28.6363\tD(x): 0.9985\tD(G(z)): 0.0000 / 0.0001\n",
      "Epoch[1890/2000]\tLoss_D: 0.0678\tLoss_G: 27.1912\tD(x): 0.9560\tD(G(z)): 0.0200 / 0.0248\n",
      "Epoch[1900/2000]\tLoss_D: 0.0173\tLoss_G: 10.4112\tD(x): 0.9851\tD(G(z)): 0.0021 / 0.0018\n",
      "Epoch[1910/2000]\tLoss_D: 0.0451\tLoss_G: 5.1868\tD(x): 0.9715\tD(G(z)): 0.0137 / 0.0088\n",
      "Epoch[1920/2000]\tLoss_D: 0.0034\tLoss_G: 14.1148\tD(x): 0.9968\tD(G(z)): 0.0001 / 0.0001\n",
      "Epoch[1930/2000]\tLoss_D: 0.0338\tLoss_G: 5.5645\tD(x): 0.9784\tD(G(z)): 0.0114 / 0.0112\n",
      "Epoch[1940/2000]\tLoss_D: 0.3606\tLoss_G: 4.4046\tD(x): 0.8822\tD(G(z)): 0.1269 / 0.0579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1950/2000]\tLoss_D: 0.2932\tLoss_G: 7.1243\tD(x): 0.8239\tD(G(z)): 0.0035 / 0.0094\n",
      "Epoch[1960/2000]\tLoss_D: 174.7217\tLoss_G: 0.9223\tD(x): 1.0000\tD(G(z)): 1.0000 / 0.4395\n",
      "Epoch[1970/2000]\tLoss_D: 1.3858\tLoss_G: 0.6814\tD(x): 0.5095\tD(G(z)): 0.5074 / 0.5062\n",
      "Epoch[1980/2000]\tLoss_D: 0.0000\tLoss_G: 314.4329\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[1990/2000]\tLoss_D: 0.0001\tLoss_G: 33.0599\tD(x): 0.9999\tD(G(z)): 0.0000 / 0.0000\n",
      "Epoch[2000/2000]\tLoss_D: 0.0003\tLoss_G: 11.5901\tD(x): 0.9997\tD(G(z)): 0.0000 / 0.0000\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "# Preliminaries : \n",
    "# 1) training allows restriction of latent codes in the l2 unit ball \n",
    "def project_l2_ball(z):\n",
    "    \"\"\" project the vectors in z onto the l2 unit norm ball\"\"\"\n",
    "    z_l2_norm = torch.norm(z, p=2, dim=0).detach()\n",
    "    if z_l2_norm.item() > 1:\n",
    "        z = z.div(z_l2_norm.expand_as(z))\n",
    "    return z\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "netG.train()\n",
    "netD.train()\n",
    "# For each epoch\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Manually build batches for current epoch. Toy code without dataloaders.\n",
    "    ids = np.asarray(np.random.permutation(np.arange(TRAINING_DATA)))\n",
    "    num_batches = TRAINING_DATA//BATCH_SIZE\n",
    "    batches = np.split(ids, num_batches)\n",
    "    # initialize loss accumulator \n",
    "    #errG_acc = 0.0\n",
    "    #errD_acc = 0.0\n",
    "    # For each batch\n",
    "    for i, ids in enumerate(batches):   \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_batch = torch.tensor(training_data[ids], device=device)\n",
    "        b_size = real_batch.size(0)\n",
    "        label_real = torch.full((b_size,), real_label, device=device) #- D_label_smoothing*torch.randn((b_size,), device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output_real = netD(real_batch).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output_real, label_real)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        nn.utils.clip_grad_norm_(netD.parameters(), clip_value)\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "        D_x = sigmoid_response(output_real).mean().item()\n",
    "\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        netD.zero_grad()\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, LATENT_SIZE, device=device)\n",
    "        #for idx in range(b_size):\n",
    "        #    noise[idx] = project_l2_ball(noise[idx]) #constrain latent codes in the unit ball\n",
    "        # Generate fake image batch with G\n",
    "        fake_batch = netG(noise)\n",
    "        label_fake = torch.full((b_size,), fake_label, device=device) #- D_label_smoothing*torch.randn((b_size,), device=device)\n",
    "        # Classify all fake batch with D\n",
    "        output_fake = netD(fake_batch.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output_fake, label_fake)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        nn.utils.clip_grad_norm_(netD.parameters(), clip_value)\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "        D_G_z1 = sigmoid_response(output_fake).mean().item()\n",
    "        # Get current error of D: Add the errors from the all-real and all-fake batches\n",
    "        errD = errD_real.item() + errD_fake.item()\n",
    "        #errD.backward()\n",
    "        # Update D\n",
    "        #optimizerD.step()\n",
    "\n",
    "        if (i+1) % D_ITERS == 0:\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.randn(b_size, LATENT_SIZE, device=device)\n",
    "            #for idx in range(b_size):\n",
    "            #    noise[idx] = project_l2_ball(noise[idx]) #constrain latent codes in the unit ball\n",
    "            # Generate fake image batch with G\n",
    "            fake_batch = netG(noise)\n",
    "            label_G = torch.full((b_size,), real_label, device=device) # fake labels are real for generator cost\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            output_G = netD(fake_batch).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = criterion(output_G, label_G)\n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            #nn.utils.clip_grad_norm_(netG.parameters(), clip_value)\n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "            D_G_z2 = sigmoid_response(output_G).mean().item()\n",
    "            # Get current error of G\n",
    "            errG = errG.item()\n",
    "        \n",
    "        #errG_acc += errG.item()\n",
    "        #errD_acc += errD.item()\n",
    "        \n",
    "        # Output training stats, for last batch\n",
    "        if ((epoch==0 or (epoch+1) % LOG_EVERY == 0)) and (i == num_batches-1):\n",
    "            print('Epoch[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "            % (epoch+1, NUM_EPOCHS, errD, errG, D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(errG)\n",
    "            D_losses.append(errD)\n",
    "\n",
    "    # Check how the generator is doing by saving G's output on fixed_noise\n",
    "    # plus Checkpoint generator\n",
    "    if ((epoch+1) % SAVE_EVERY == 0):\n",
    "        with torch.no_grad():\n",
    "            fake_batch_checkpoint = netG(fixed_noise).detach().cpu()\n",
    "        # save checkpoints\n",
    "        #torch.save(fake, osp.join(result_folder,\"checkpoint_sample_%d.pth\")%(epoch+1))\n",
    "        # save learned model so far\n",
    "        torch.save(netG.state_dict(), osp.join(result_folder,\"learned_generator_%d.pth\")%(epoch+1))\n",
    "        \n",
    "# save the learned model and stats at the end of training \n",
    "torch.save(netG.state_dict(), osp.join(result_folder,\"learned_model.pth\"))\n",
    "np.save(osp.join(result_folder,\"G_losses.npy\"), np.asarray(G_losses))\n",
    "np.save(osp.join(result_folder,\"D_losses.npy\"), np.asarray(D_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
